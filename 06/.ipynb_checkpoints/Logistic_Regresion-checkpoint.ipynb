{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "import PIL\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 10)\n"
     ]
    }
   ],
   "source": [
    "image = tf.Variable(tf.zeros((28,28)))\n",
    "dummy = mnist.train.images[0]\n",
    "x_ = tf.placeholder(tf.float32, (28,28))\n",
    "\n",
    "img = dummy.reshape((28,28))\n",
    "\n",
    "x_hat = image\n",
    "assign_op = tf.assign(x_hat, x_)\n",
    "\n",
    "epsilon = tf.placeholder(tf.float32, ())\n",
    "\n",
    "below = x_ - epsilon \n",
    "above = x_ + epsilon\n",
    "projected = tf.clip_by_value(tf.clip_by_value(x_hat, below, above), 0, 1)\n",
    "\n",
    "with tf.control_dependencies([projected]):\n",
    "    project_step = tf.assign(x_hat,projected)\n",
    "    \n",
    "demo_epsilon = 0.2\n",
    "demo_learning = 1e-1\n",
    "dummy = dummy[np.newaxis,:]\n",
    "dummys_l = mnist.train.labels[1]\n",
    "dummy_l = dummys_l[np.newaxis,:]\n",
    "print(dummy_l.shape)\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(assign_op, feed_dict={x_: img})\n",
    "\n",
    "for i in range(100):\n",
    "    sess.run(project_step, feed_dict={x_: img, epsilon: demo_epsilon})\n",
    "#     _, loss = sess.run([optimizer, cost], feed_dict={x:dummy , y:dummys_l})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 784)\n",
      "Epoch: 0001 cost= 2.302585125\n",
      "Optimization Finished!\n",
      "Accuracy: 1.0\n",
      "[[[0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.39039218 0.38647062 0.3119608\n",
      "   0.47274512 0.2492157  0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.36294118\n",
      "   0.5511765  0.9315687  0.9315687  0.9315687  0.9315687  0.9315687\n",
      "   0.9315687  0.9943138  0.9943138  0.9825491  1.         0.97078437\n",
      "   0.9315687  0.75509804 0.09235294 0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.5590196  0.9943138  1.         1.\n",
      "   1.         1.         1.         1.         1.         1.\n",
      "   1.         1.         1.         1.         1.         1.\n",
      "   0.7511765  0.10019608 0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.89627457 1.         0.82568634 0.7903922  0.7903922  0.7903922\n",
      "   0.7903922  0.55509806 0.2492157  0.2492157  0.2492157  0.2492157\n",
      "   0.2492157  0.5119608  0.8805883  1.         1.         0.7511765\n",
      "   0.09235294 0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.15901962 0.33156863\n",
      "   0.06098039 0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.14333335 0.8452942  1.         1.         0.46098042 0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.33941177\n",
      "   1.         1.         0.9276471  0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.33941177 1.         1.\n",
      "   0.9276471  0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.4256863  0.6256863  1.         1.         0.9629412  0.21000002\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.10803922\n",
      "   0.46882355 0.9041177  0.9041177  0.9041177  1.         1.\n",
      "   1.         1.         1.         0.9511765  0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.27666667 0.4766667  0.87274516 1.         1.\n",
      "   1.         1.         1.         1.         1.         1.\n",
      "   1.         0.56686276 0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.15509805 0.74333334 1.\n",
      "   1.         1.         1.         0.88450986 0.8178432  0.8178432\n",
      "   0.30411765 0.27666667 0.8531373  1.         1.         0.46882355\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.45313728 0.8688236  1.         0.95901966 0.90019614 0.46098042\n",
      "   0.3590196  0.13156864 0.         0.         0.         0.\n",
      "   0.7943138  1.         0.9550981  0.17078432 0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.6727451  1.\n",
      "   0.7001961  0.25313726 0.         0.         0.         0.\n",
      "   0.         0.         0.         0.19823532 0.9158824  1.\n",
      "   0.9276471  0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.08058824 0.49627453 0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.33941177 1.         1.         0.6609804  0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.55509806\n",
      "   1.         0.9433334  0.23352943 0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.8335295  0.9903922  1.         0.66882354\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.95901966 1.         0.94725496 0.23352943 0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.3590196  0.9943138  0.9550981\n",
      "   0.3472549  0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.02960784 0.8178432  0.97470593 0.6256863  0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.02568628 0.46882355\n",
      "   0.28058824 0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.        ]]]\n"
     ]
    }
   ],
   "source": [
    "#Hyperparameters\n",
    "learning_rate = 0.01\n",
    "training_epochs = 1\n",
    "\n",
    "\n",
    "# Single image classification\n",
    "x = tf.placeholder(tf.float32, (None, 784)) # mnist data image of shape 28*28=784\n",
    "y = tf.placeholder(tf.float32, [None, 10]) # 0-9 digits recognition => 10 classes\n",
    "x_image = tf.reshape(x, [-1,28,28,1]) # mnist image comes in as 784 vector\n",
    "\n",
    "\n",
    "# Set model weights\n",
    "W = tf.Variable(tf.zeros([784, 10]))\n",
    "b = tf.Variable(tf.zeros([10]))\n",
    "\n",
    "# Construct model\n",
    "pred = tf.nn.softmax(tf.matmul(x, W) + b) # Softmax\n",
    "\n",
    "# Minimize error using cross entropy\n",
    "cost = tf.reduce_mean(-tf.reduce_sum(y*tf.log(pred), reduction_indices=1))\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "# Gradient Descent\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\n",
    "\n",
    "deriv = tf.gradients(cost, x)\n",
    "image_adv = tf.stop_gradient(x - tf.sign(deriv)*learning_rate)\n",
    "image_adv = tf.clip_by_value(image_adv, 0, 1)\n",
    "\n",
    "# Initialize the variables (i.e. assign their default value)\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Start training\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    # Run the initializer\n",
    "    sess.run(init)\n",
    "    \n",
    "    #Image to train\n",
    "    dummy = mnist.train.images[0]\n",
    "    dummy = np.reshape(dummy,(1,784))\n",
    "    dummy_l = mnist.train.labels[0]\n",
    "    print(dummy.shape)\n",
    "    dummy_l = dummy_l[np.newaxis,:]\n",
    "    \n",
    "    #Image to test\n",
    "    dummy2 = mnist.train.images[0]\n",
    "    dummy2 = dummy2[np.newaxis,:]\n",
    "    dummy2_l = mnist.train.labels[0]\n",
    "    dummy2_l = dummy2_l[np.newaxis,:]\n",
    "        \n",
    "    # Run optimization op (backprop) and cost op (to get loss value)\n",
    "    _, c = sess.run([optimizer, cost], feed_dict={x: dummy,\n",
    "                                                          y: dummy_l})\n",
    "        \n",
    "    print(\"Epoch:\", '%04d' % (1), \"cost=\", \"{:.9f}\".format(c))\n",
    "    print(\"Optimization Finished!\")\n",
    "\n",
    "    # Test model\n",
    "    correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    \n",
    "#     print(\"Accuracy:\", accuracy.eval({x: mnist.test.images, y: mnist.test.labels}))\n",
    "    print(\"Accuracy:\", accuracy.eval({x: dummy2, y: dummy2_l}))\n",
    "    \n",
    "    #Computes gradients, I think or hope!\n",
    "    dydx = sess.run(deriv,  feed_dict = {x: dummy, y:dummy_l}) # can't seem to access 'deriv' w/o running this\n",
    "#     _ = sess.run([grad_input, cost], feed_dict={x: dummy,\n",
    "#                                                           y: dummy_l})\n",
    "    x_adv = sess.run(image_adv, {x: dummy, y:dummy_l})\n",
    "    \n",
    "    print (x_adv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"transpose_2:0\", shape=(6,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "tf.image?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost= 1.183312914\n",
      "Epoch: 0002 cost= 0.665077362\n",
      "Epoch: 0003 cost= 0.552732731\n",
      "Epoch: 0004 cost= 0.498695973\n",
      "Epoch: 0005 cost= 0.465296411\n",
      "Epoch: 0006 cost= 0.442628961\n",
      "Epoch: 0007 cost= 0.425678634\n",
      "Epoch: 0008 cost= 0.412030358\n",
      "Epoch: 0009 cost= 0.401618818\n",
      "Epoch: 0010 cost= 0.392194463\n",
      "Epoch: 0011 cost= 0.384686858\n",
      "Epoch: 0012 cost= 0.378191893\n",
      "Epoch: 0013 cost= 0.372338644\n",
      "Epoch: 0014 cost= 0.367285049\n",
      "Epoch: 0015 cost= 0.362729420\n",
      "Epoch: 0016 cost= 0.358750533\n",
      "Epoch: 0017 cost= 0.354745524\n",
      "Epoch: 0018 cost= 0.351432862\n",
      "Epoch: 0019 cost= 0.348353285\n",
      "Epoch: 0020 cost= 0.345406492\n",
      "Epoch: 0021 cost= 0.342725566\n",
      "Epoch: 0022 cost= 0.340326975\n",
      "Epoch: 0023 cost= 0.337951536\n",
      "Epoch: 0024 cost= 0.335844693\n",
      "Epoch: 0025 cost= 0.333530496\n",
      "Optimization Finished!\n",
      "Accuracy: 0.9137\n"
     ]
    }
   ],
   "source": [
    "#LOGISTIC REGRESSION\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.01\n",
    "training_epochs = 25\n",
    "batch_size = 100\n",
    "display_step = 1\n",
    "\n",
    "# tf Graph Input\n",
    "x = tf.placeholder(tf.float32, [None, 784]) # mnist data image of shape 28*28=784\n",
    "y = tf.placeholder(tf.float32, [None, 10]) # 0-9 digits recognition => 10 classes\n",
    "\n",
    "# Set model weights\n",
    "W = tf.Variable(tf.zeros([784, 10]))\n",
    "b = tf.Variable(tf.zeros([10]))\n",
    "\n",
    "# Construct model\n",
    "pred = tf.nn.softmax(tf.matmul(x, W) + b) # Softmax\n",
    "\n",
    "# Minimize error using cross entropy\n",
    "cost = tf.reduce_mean(-tf.reduce_sum(y*tf.log(pred), reduction_indices=1))\n",
    "\n",
    "# Gradient Descent\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\n",
    "\n",
    "# Initialize the variables (i.e. assign their default value)\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Start training\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    # Run the initializer\n",
    "    sess.run(init)\n",
    "\n",
    "    # Training cycle\n",
    "    for epoch in range(training_epochs):\n",
    "        \n",
    "        avg_cost = 0.\n",
    "        total_batch = int(mnist.train.num_examples/batch_size)\n",
    "        \n",
    "        # Loop over all batches\n",
    "        for i in range(total_batch):\n",
    "            \n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "            # Run optimization op (backprop) and cost op (to get loss value)\n",
    "            _, c = sess.run([optimizer, cost], feed_dict={x: batch_xs,\n",
    "                                                          y: batch_ys})\n",
    "            # Compute average loss\n",
    "            avg_cost += c / total_batch\n",
    "        # Display logs per epoch step\n",
    "        \n",
    "        if (epoch+1) % display_step == 0:\n",
    "            print(\"Epoch:\", '%04d' % (epoch+1), \"cost=\", \"{:.9f}\".format(avg_cost))\n",
    "    print(\"Optimization Finished!\")\n",
    "\n",
    "    # Test model\n",
    "    correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    \n",
    "    print(\"Accuracy:\", accuracy.eval({x: mnist.test.images, y: mnist.test.labels}))\n",
    "\n",
    "#https://github.com/aymericdamien/TensorFlow-Examples/blob/master/examples/2_BasicModels/logistic_regression.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.argmax?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This creates a simple linear model and defines a regression loss, \n",
    "then you can easily define the gradient of the loss with respect of \n",
    "the input (x). Then you just need to wrap this into a \n",
    "TensorFlow session in order to train the model and run \n",
    "the code (like adding optimizers and training the model). \n",
    "Note that still you need to add a classification loss to \n",
    "this code to make it work properly, but the idea is to point you in the right direction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'Tensor' object is not iterable.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-79-1abd7480751f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mgrad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGradientDescentOptimizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompute_gradients\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcost\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mgrad_input\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGradientDescentOptimizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcost\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# tf Graph Input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# x = tf.placeholder(tf.float32, [None, 784]) # mnist data image of shape 28*28=784\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\training\\optimizer.py\u001b[0m in \u001b[0;36mapply_gradients\u001b[1;34m(self, grads_and_vars, global_step, name)\u001b[0m\n\u001b[0;32m    419\u001b[0m     \u001b[1;31m# methods: _create_slots(), _prepare(), _apply_dense(), and _apply_sparse().\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    420\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 421\u001b[1;33m     \u001b[0mgrads_and_vars\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrads_and_vars\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Make sure repeat iteration works.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    422\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mgrads_and_vars\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    423\u001b[0m       \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"No variables provided.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    502\u001b[0m       \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mwhen\u001b[0m \u001b[0minvoked\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    503\u001b[0m     \"\"\"\n\u001b[1;32m--> 504\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"'Tensor' object is not iterable.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    505\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    506\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m__bool__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'Tensor' object is not iterable."
     ]
    }
   ],
   "source": [
    "#grad = tf.train.GradientDescentOptimizer(image).compute_gradients(cost)\n",
    "# tf Graph Input\n",
    "# x = tf.placeholder(tf.float32, [None, 784]) # mnist data image of shape 28*28=784\n",
    "# y = tf.placeholder(tf.float32, [None, 10]) # 0-9 digits recognition => 10 classes\n",
    "\n",
    "#Getting an image\n",
    "dummy = mnist.train.images[0]\n",
    "x_ = dummy[np.newaxis,:]\n",
    "dummy_l = mnist.train.labels[0]\n",
    "y_ = dummy_l[np.newaxis,:]\n",
    "\n",
    "# Set model weights\n",
    "W = tf.Variable(tf.zeros([784, 10]))\n",
    "b = tf.Variable(tf.zeros([10]))\n",
    "\n",
    "# Construct model\n",
    "pred = tf.nn.softmax(tf.matmul(x_, W) + b) # Softmax\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fea168d6a58>"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAADZxJREFUeJzt3X+IHPUZx/HPkx9VuFRMGnMETbUt0lBPSMsRK2o5sSkqxRj/iFEoKZaeQhMrKjRaSAOCaOkPCmLxiqGxJDZKW5M/qq0NjT+wiLlo/d2q9RovXhIlEg1iYuLTP24sp958Z92d3Zm95/2C43bnmR8Pm3xuZnd25mvuLgDxTKu6AQDVIPxAUIQfCIrwA0ERfiAowg8ERfiBoAg/EBThB4Ka0cmNmRlfJwTazN2tkfla2vOb2flm9i8ze9nM1rSyLgCdZc1+t9/Mpkv6t6QlkkYlPSHpMnd/PrEMe36gzTqx518s6WV3/4+7H5b0e0lLW1gfgA5qJfwnSnptwvPRbNpHmNmgme0wsx0tbAtAydr+gZ+7D0kakjjsB+qklT3/bkkLJjw/KZsGoAu0Ev4nJJ1qZl8ws89IWiFpazltAWi3pg/73f2Ima2S9BdJ0yWtd/fnSusMQFs1faqvqY3xnh9ou458yQdA9yL8QFCEHwiK8ANBEX4gKMIPBEX4gaAIPxAU4QeCIvxAUIQfCIrwA0ERfiAowg8ERfiBoAg/EBThB4Ii/EBQhB8IivADQRF+ICjCDwRF+IGgCD8QFOEHgiL8QFCEHwiK8ANBEX4gqKaH6JYkMxuR9I6ko5KOuHt/GU0BaL+Wwp85193fLGE9ADqIw34gqFbD75L+ambDZjZYRkMAOqPVw/6z3X23mc2T9KCZvejuD0+cIfujwB8GoGbM3ctZkdk6SQfd/WeJecrZGIBc7m6NzNf0Yb+Z9ZjZZz98LOlbkp5tdn0AOquVw/5eSX8ysw/Xs8ndHyilKwBtV9phf0Mb47AfaLu2H/YD6G6EHwiK8ANBEX4gKMIPBEX4gaDKuKoPFbv++utza0Wnct98M31BZl9fX7L+yCOPJOtbt25N1lEd9vxAUIQfCIrwA0ERfiAowg8ERfiBoAg/ENSUuaR39erVyfoZZ5yRrF9yySVlttNRxxxzTNPLFv37T58+PVl///33k/UjR47k1nbt2pVcdmBgIFnfs2dPsh4Vl/QCSCL8QFCEHwiK8ANBEX4gKMIPBEX4gaC66jz/pk2bcmuXXnppctlp0/g7121efPHFZP28885L1l9//fUy2+kanOcHkET4gaAIPxAU4QeCIvxAUIQfCIrwA0EVnuc3s/WSvi1pn7v3ZdPmSNos6RRJI5KWu/tbhRtr8Tz/gQMHcmvHHXdcctmic76HDx9uqqcyPPbYY8n65s2bO9TJp3fBBRck6ytWrMitHX/88S1tu+h7AOeee25ubSrfC6DM8/y/lXT+x6atkbTN3U+VtC17DqCLFIbf3R+WtP9jk5dK2pA93iDp4pL7AtBmzb7n73X3sezxHkm9JfUDoENaHqvP3T31Xt7MBiUNtrodAOVqds+/18zmS1L2e1/ejO4+5O797t7f5LYAtEGz4d8qaWX2eKWkLeW0A6BTCsNvZndL+oekL5vZqJl9T9ItkpaY2UuSvpk9B9BFuup6/tNPPz23VnRf/nvvvTdZT32HAM1buHBhbu2hhx5KLjtv3ryWtn3rrbfm1tasmbpnp7meH0AS4QeCIvxAUIQfCIrwA0ERfiCorjrVh6llcDD9re877rijpfW/++67ubWenp6W1l1nnOoDkET4gaAIPxAU4QeCIvxAUIQfCIrwA0ERfiAowg8ERfiBoAg/EBThB4Ii/EBQhB8IivADQbU8XBeQsnbt2tzaOeec09Ztz5iR/997YGAguez27dvLbaaG2PMDQRF+ICjCDwRF+IGgCD8QFOEHgiL8QFCF9+03s/WSvi1pn7v3ZdPWSfq+pDey2W509z8Xboz79rfFggULcmurV69OLnvVVVeV3c5HzJo1K7dm1tDt5dvi0KFDyfqxxx7boU7KV+Z9+38r6fxJpv/S3RdlP4XBB1AvheF394cl7e9ALwA6qJX3/KvM7GkzW29ms0vrCEBHNBv+X0v6kqRFksYk/TxvRjMbNLMdZrajyW0BaIOmwu/ue939qLt/IOk3khYn5h1y935372+2SQDlayr8ZjZ/wtNlkp4tpx0AnVJ4Sa+Z3S1pQNJcMxuV9BNJA2a2SJJLGpF0ZRt7BNAGheF398smmXxnG3oJa/ny5cn64sW576okSVdccUVubfZsPoudzH333Vd1C5XjG35AUIQfCIrwA0ERfiAowg8ERfiBoLh1dwn6+vqS9XvuuSdZX7hwYbLezktfDxw4kKwfPHiwpfXfcMMNubWiy2pvu+22ZP2EE05oqidJ2rVrV9PLThXs+YGgCD8QFOEHgiL8QFCEHwiK8ANBEX4gqMJbd5e6sS6+dffNN9+cW7vyyvTtDObMmZOsHz58OFkvOh9+++2359ZGR0eTy95///3J+iuvvJKst9PIyEiyfvLJJyfrqdftzDPPTC775JNPJut1VuatuwFMQYQfCIrwA0ERfiAowg8ERfiBoAg/EBTX8zdoYGAgt1Z0Hn94eDhZv+mmm5L1LVu2JOvd6qyzzkrW586d29L6jx49mlvr5vP4ZWHPDwRF+IGgCD8QFOEHgiL8QFCEHwiK8ANBFZ7nN7MFku6S1CvJJQ25+6/MbI6kzZJOkTQiabm7v9W+Vqt10UUX5dbWrl2bXPbqq68uu50p4bTTTkvWe3p6Wlr/zp07W1p+qmtkz39E0nXu/hVJX5f0AzP7iqQ1kra5+6mStmXPAXSJwvC7+5i778wevyPpBUknSloqaUM22wZJF7erSQDl+1Tv+c3sFElflfS4pF53H8tKezT+tgBAl2j4u/1mNkvSHyRd4+5vTxw/zt097/58ZjYoabDVRgGUq6E9v5nN1HjwN7r7H7PJe81sflafL2nfZMu6+5C797t7fxkNAyhHYfhtfBd/p6QX3P0XE0pbJa3MHq+UNDUvPQOmqMJbd5vZ2ZIekfSMpA+yyTdq/H3/PZI+L+m/Gj/Vt79gXV17626Ub+PGjcn65Zdfnqy/9957yfqyZctyaw888EBy2W7W6K27C9/zu/ujkvJWdt6naQpAffANPyAowg8ERfiBoAg/EBThB4Ii/EBQ3LobbTU2NpZbmzdvXkvrLrpkdyqfyy8De34gKMIPBEX4gaAIPxAU4QeCIvxAUIQfCIrz/Gir1PDl06al9z1F1+sXDW2ONPb8QFCEHwiK8ANBEX4gKMIPBEX4gaAIPxAU5/nRklWrViXrM2bk/xc7dOhQctlrr702Wed6/daw5weCIvxAUIQfCIrwA0ERfiAowg8ERfiBoMzd0zOYLZB0l6ReSS5pyN1/ZWbrJH1f0hvZrDe6+58L1pXeGGpn5syZyfqrr76arPf29ubWtm/fnlx2yZIlyTom5+7WyHyNfMnniKTr3H2nmX1W0rCZPZjVfunuP2u2SQDVKQy/u49JGssev2NmL0g6sd2NAWivT/We38xOkfRVSY9nk1aZ2dNmtt7MZucsM2hmO8xsR0udAihVw+E3s1mS/iDpGnd/W9KvJX1J0iKNHxn8fLLl3H3I3fvdvb+EfgGUpKHwm9lMjQd/o7v/UZLcfa+7H3X3DyT9RtLi9rUJoGyF4Tczk3SnpBfc/RcTps+fMNsySc+W3x6Admnk0/6zJH1H0jNm9lQ27UZJl5nZIo2f/huRdGVbOkSlik4Fb9q0KVkfHh7OrW3evLmpnlCORj7tf1TSZOcNk+f0AdQb3/ADgiL8QFCEHwiK8ANBEX4gKMIPBFV4SW+pG+OSXqDtGr2klz0/EBThB4Ii/EBQhB8IivADQRF+ICjCDwTV6SG635T03wnP52bT6qiuvdW1L4nemlVmbyc3OmNHv+TziY2b7ajrvf3q2ltd+5LorVlV9cZhPxAU4QeCqjr8QxVvP6WuvdW1L4nemlVJb5W+5wdQnar3/AAqUkn4zex8M/uXmb1sZmuq6CGPmY2Y2TNm9lTVQ4xlw6DtM7NnJ0ybY2YPmtlL2e9Jh0mrqLd1ZrY7e+2eMrMLK+ptgZn93cyeN7PnzOyH2fRKX7tEX5W8bh0/7Dez6ZL+LWmJpFFJT0i6zN2f72gjOcxsRFK/u1d+TtjMviHpoKS73L0vm/ZTSfvd/ZbsD+dsd/9RTXpbJ+lg1SM3ZwPKzJ84srSkiyV9VxW+dom+lquC162KPf9iSS+7+3/c/bCk30taWkEftefuD0va/7HJSyVtyB5v0Ph/no7L6a0W3H3M3Xdmj9+R9OHI0pW+dom+KlFF+E+U9NqE56Oq15DfLumvZjZsZoNVNzOJ3mzYdEnaI6m3ymYmUThycyd9bGTp2rx2zYx4XTY+8Puks939a5IukPSD7PC2lnz8PVudTtc0NHJzp0wysvT/VfnaNTviddmqCP9uSQsmPD8pm1YL7r47+71P0p9Uv9GH9344SGr2e1/F/fxfnUZunmxkadXgtavTiNdVhP8JSaea2RfM7DOSVkjaWkEfn2BmPdkHMTKzHknfUv1GH94qaWX2eKWkLRX28hF1Gbk5b2RpVfza1W7Ea3fv+I+kCzX+if8rkn5cRQ85fX1R0j+zn+eq7k3S3Ro/DHxf45+NfE/S5yRtk/SSpL9JmlOj3n4n6RlJT2s8aPMr6u1sjR/SPy3pqeznwqpfu0RflbxufMMPCIoP/ICgCD8QFOEHgiL8QFCEHwiK8ANBEX4gKMIPBPU/uUhluG6K4TwAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Ploting image\n",
    "image1_= batch_xs\n",
    "image2_ = mnist.test.images[1]\n",
    "\n",
    "plt.imshow(image2_.reshape((28,28)), cmap='Greys')\n",
    "\n",
    "plt.imshow(img, cmap='Greys_r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 784)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fea1688d940>"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "adv_ = adv.reshape(1,784)\n",
    "print(adv_.shape)\n",
    "plt.imshow(adv_.reshape((28,28)), cmap='Greys')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(mnist.train.labels[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
