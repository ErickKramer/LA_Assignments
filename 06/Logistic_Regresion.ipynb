{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "import PIL\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 10)\n"
     ]
    }
   ],
   "source": [
    "image = tf.Variable(tf.zeros((28,28)))\n",
    "dummy = mnist.train.images[0]\n",
    "x_ = tf.placeholder(tf.float32, (28,28))\n",
    "\n",
    "img = dummy.reshape((28,28))\n",
    "\n",
    "x_hat = image\n",
    "assign_op = tf.assign(x_hat, x_)\n",
    "\n",
    "epsilon = tf.placeholder(tf.float32, ())\n",
    "\n",
    "below = x_ - epsilon \n",
    "above = x_ + epsilon\n",
    "projected = tf.clip_by_value(tf.clip_by_value(x_hat, below, above), 0, 1)\n",
    "\n",
    "with tf.control_dependencies([projected]):\n",
    "    project_step = tf.assign(x_hat,projected)\n",
    "    \n",
    "demo_epsilon = 0.2\n",
    "demo_learning = 1e-1\n",
    "dummy = dummy[np.newaxis,:]\n",
    "dummys_l = mnist.train.labels[1]\n",
    "dummy_l = dummys_l[np.newaxis,:]\n",
    "print(dummy_l.shape)\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(assign_op, feed_dict={x_: img})\n",
    "\n",
    "for i in range(100):\n",
    "    sess.run(project_step, feed_dict={x_: img, epsilon: demo_epsilon})\n",
    "#     _, loss = sess.run([optimizer, cost], feed_dict={x:dummy , y:dummys_l})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 784)\n",
      "Epoch: 0001 cost= 2.302585125\n",
      "Optimization Finished!\n",
      "Accuracy: 1.0\n",
      "[[[0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.39039218 0.38647062 0.3119608\n",
      "   0.47274512 0.2492157  0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.36294118\n",
      "   0.5511765  0.9315687  0.9315687  0.9315687  0.9315687  0.9315687\n",
      "   0.9315687  0.9943138  0.9943138  0.9825491  1.         0.97078437\n",
      "   0.9315687  0.75509804 0.09235294 0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.5590196  0.9943138  1.         1.\n",
      "   1.         1.         1.         1.         1.         1.\n",
      "   1.         1.         1.         1.         1.         1.\n",
      "   0.7511765  0.10019608 0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.89627457 1.         0.82568634 0.7903922  0.7903922  0.7903922\n",
      "   0.7903922  0.55509806 0.2492157  0.2492157  0.2492157  0.2492157\n",
      "   0.2492157  0.5119608  0.8805883  1.         1.         0.7511765\n",
      "   0.09235294 0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.15901962 0.33156863\n",
      "   0.06098039 0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.14333335 0.8452942  1.         1.         0.46098042 0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.33941177\n",
      "   1.         1.         0.9276471  0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.33941177 1.         1.\n",
      "   0.9276471  0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.4256863  0.6256863  1.         1.         0.9629412  0.21000002\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.10803922\n",
      "   0.46882355 0.9041177  0.9041177  0.9041177  1.         1.\n",
      "   1.         1.         1.         0.9511765  0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.27666667 0.4766667  0.87274516 1.         1.\n",
      "   1.         1.         1.         1.         1.         1.\n",
      "   1.         0.56686276 0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.15509805 0.74333334 1.\n",
      "   1.         1.         1.         0.88450986 0.8178432  0.8178432\n",
      "   0.30411765 0.27666667 0.8531373  1.         1.         0.46882355\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.45313728 0.8688236  1.         0.95901966 0.90019614 0.46098042\n",
      "   0.3590196  0.13156864 0.         0.         0.         0.\n",
      "   0.7943138  1.         0.9550981  0.17078432 0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.6727451  1.\n",
      "   0.7001961  0.25313726 0.         0.         0.         0.\n",
      "   0.         0.         0.         0.19823532 0.9158824  1.\n",
      "   0.9276471  0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.08058824 0.49627453 0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.33941177 1.         1.         0.6609804  0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.55509806\n",
      "   1.         0.9433334  0.23352943 0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.8335295  0.9903922  1.         0.66882354\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.95901966 1.         0.94725496 0.23352943 0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.3590196  0.9943138  0.9550981\n",
      "   0.3472549  0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.02960784 0.8178432  0.97470593 0.6256863  0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.02568628 0.46882355\n",
      "   0.28058824 0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.        ]]]\n"
     ]
    }
   ],
   "source": [
    "#Hyperparameters\n",
    "learning_rate = 0.01\n",
    "training_epochs = 1\n",
    "\n",
    "\n",
    "# Single image classification\n",
    "x = tf.placeholder(tf.float32, (None, 784)) # mnist data image of shape 28*28=784\n",
    "y = tf.placeholder(tf.float32, [None, 10]) # 0-9 digits recognition => 10 classes\n",
    "x_image = tf.reshape(x, [-1,28,28,1]) # mnist image comes in as 784 vector\n",
    "\n",
    "\n",
    "# Set model weights\n",
    "W = tf.Variable(tf.zeros([784, 10]))\n",
    "b = tf.Variable(tf.zeros([10]))\n",
    "\n",
    "# Construct model\n",
    "pred = tf.nn.softmax(tf.matmul(x, W) + b) # Softmax\n",
    "\n",
    "# Minimize error using cross entropy\n",
    "cost = tf.reduce_mean(-tf.reduce_sum(y*tf.log(pred), reduction_indices=1))\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "# Gradient Descent\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\n",
    "\n",
    "deriv = tf.gradients(cost, x)\n",
    "image_adv = tf.stop_gradient(x - tf.sign(deriv)*learning_rate)\n",
    "image_adv = tf.clip_by_value(image_adv, 0, 1)\n",
    "\n",
    "# Initialize the variables (i.e. assign their default value)\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Start training\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    # Run the initializer\n",
    "    sess.run(init)\n",
    "    \n",
    "    #Image to train\n",
    "    dummy = mnist.train.images[0]\n",
    "    dummy = np.reshape(dummy,(1,784))\n",
    "    dummy_l = mnist.train.labels[0]\n",
    "    print(dummy.shape)\n",
    "    dummy_l = dummy_l[np.newaxis,:]\n",
    "    \n",
    "    #Image to test\n",
    "    dummy2 = mnist.train.images[0]\n",
    "    dummy2 = dummy2[np.newaxis,:]\n",
    "    dummy2_l = mnist.train.labels[0]\n",
    "    dummy2_l = dummy2_l[np.newaxis,:]\n",
    "        \n",
    "    # Run optimization op (backprop) and cost op (to get loss value)\n",
    "    _, c = sess.run([optimizer, cost], feed_dict={x: dummy,\n",
    "                                                          y: dummy_l})\n",
    "        \n",
    "    print(\"Epoch:\", '%04d' % (1), \"cost=\", \"{:.9f}\".format(c))\n",
    "    print(\"Optimization Finished!\")\n",
    "\n",
    "    # Test model\n",
    "    correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    \n",
    "#     print(\"Accuracy:\", accuracy.eval({x: mnist.test.images, y: mnist.test.labels}))\n",
    "    print(\"Accuracy:\", accuracy.eval({x: dummy2, y: dummy2_l}))\n",
    "    \n",
    "    #Computes gradients, I think or hope!\n",
    "    dydx = sess.run(deriv,  feed_dict = {x: dummy, y:dummy_l}) # can't seem to access 'deriv' w/o running this\n",
    "#     _ = sess.run([grad_input, cost], feed_dict={x: dummy,\n",
    "#                                                           y: dummy_l})\n",
    "    x_adv = sess.run(image_adv, {x: dummy, y:dummy_l})\n",
    "    \n",
    "    print (x_adv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"transpose_2:0\", shape=(6,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "tf.image?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost= 1.183312914\n",
      "Epoch: 0002 cost= 0.665077362\n",
      "Epoch: 0003 cost= 0.552732731\n",
      "Epoch: 0004 cost= 0.498695973\n",
      "Epoch: 0005 cost= 0.465296411\n",
      "Epoch: 0006 cost= 0.442628961\n",
      "Epoch: 0007 cost= 0.425678634\n",
      "Epoch: 0008 cost= 0.412030358\n",
      "Epoch: 0009 cost= 0.401618818\n",
      "Epoch: 0010 cost= 0.392194463\n",
      "Epoch: 0011 cost= 0.384686858\n",
      "Epoch: 0012 cost= 0.378191893\n",
      "Epoch: 0013 cost= 0.372338644\n",
      "Epoch: 0014 cost= 0.367285049\n",
      "Epoch: 0015 cost= 0.362729420\n",
      "Epoch: 0016 cost= 0.358750533\n",
      "Epoch: 0017 cost= 0.354745524\n",
      "Epoch: 0018 cost= 0.351432862\n",
      "Epoch: 0019 cost= 0.348353285\n",
      "Epoch: 0020 cost= 0.345406492\n",
      "Epoch: 0021 cost= 0.342725566\n",
      "Epoch: 0022 cost= 0.340326975\n",
      "Epoch: 0023 cost= 0.337951536\n",
      "Epoch: 0024 cost= 0.335844693\n",
      "Epoch: 0025 cost= 0.333530496\n",
      "Optimization Finished!\n",
      "Accuracy: 0.9137\n"
     ]
    }
   ],
   "source": [
    "#LOGISTIC REGRESSION\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.01\n",
    "training_epochs = 25\n",
    "batch_size = 100\n",
    "display_step = 1\n",
    "\n",
    "# tf Graph Input\n",
    "x = tf.placeholder(tf.float32, [None, 784]) # mnist data image of shape 28*28=784\n",
    "y = tf.placeholder(tf.float32, [None, 10]) # 0-9 digits recognition => 10 classes\n",
    "\n",
    "# Set model weights\n",
    "W = tf.Variable(tf.zeros([784, 10]))\n",
    "b = tf.Variable(tf.zeros([10]))\n",
    "\n",
    "# Construct model\n",
    "pred = tf.nn.softmax(tf.matmul(x, W) + b) # Softmax\n",
    "\n",
    "# Minimize error using cross entropy\n",
    "cost = tf.reduce_mean(-tf.reduce_sum(y*tf.log(pred), reduction_indices=1))\n",
    "\n",
    "# Gradient Descent\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\n",
    "\n",
    "# Initialize the variables (i.e. assign their default value)\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Start training\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    # Run the initializer\n",
    "    sess.run(init)\n",
    "\n",
    "    # Training cycle\n",
    "    for epoch in range(training_epochs):\n",
    "        \n",
    "        avg_cost = 0.\n",
    "        total_batch = int(mnist.train.num_examples/batch_size)\n",
    "        \n",
    "        # Loop over all batches\n",
    "        for i in range(total_batch):\n",
    "            \n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "            # Run optimization op (backprop) and cost op (to get loss value)\n",
    "            _, c = sess.run([optimizer, cost], feed_dict={x: batch_xs,\n",
    "                                                          y: batch_ys})\n",
    "            # Compute average loss\n",
    "            avg_cost += c / total_batch\n",
    "        # Display logs per epoch step\n",
    "        \n",
    "        if (epoch+1) % display_step == 0:\n",
    "            print(\"Epoch:\", '%04d' % (epoch+1), \"cost=\", \"{:.9f}\".format(avg_cost))\n",
    "    print(\"Optimization Finished!\")\n",
    "\n",
    "    # Test model\n",
    "    correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    \n",
    "    print(\"Accuracy:\", accuracy.eval({x: mnist.test.images, y: mnist.test.labels}))\n",
    "\n",
    "#https://github.com/aymericdamien/TensorFlow-Examples/blob/master/examples/2_BasicModels/logistic_regression.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.argmax?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This creates a simple linear model and defines a regression loss, \n",
    "then you can easily define the gradient of the loss with respect of \n",
    "the input (x). Then you just need to wrap this into a \n",
    "TensorFlow session in order to train the model and run \n",
    "the code (like adding optimizers and training the model). \n",
    "Note that still you need to add a classification loss to \n",
    "this code to make it work properly, but the idea is to point you in the right direction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'Tensor' object is not iterable.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-79-1abd7480751f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mgrad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGradientDescentOptimizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompute_gradients\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcost\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mgrad_input\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGradientDescentOptimizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcost\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# tf Graph Input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# x = tf.placeholder(tf.float32, [None, 784]) # mnist data image of shape 28*28=784\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\training\\optimizer.py\u001b[0m in \u001b[0;36mapply_gradients\u001b[1;34m(self, grads_and_vars, global_step, name)\u001b[0m\n\u001b[0;32m    419\u001b[0m     \u001b[1;31m# methods: _create_slots(), _prepare(), _apply_dense(), and _apply_sparse().\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    420\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 421\u001b[1;33m     \u001b[0mgrads_and_vars\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrads_and_vars\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Make sure repeat iteration works.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    422\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mgrads_and_vars\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    423\u001b[0m       \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"No variables provided.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    502\u001b[0m       \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mwhen\u001b[0m \u001b[0minvoked\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    503\u001b[0m     \"\"\"\n\u001b[1;32m--> 504\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"'Tensor' object is not iterable.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    505\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    506\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m__bool__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'Tensor' object is not iterable."
     ]
    }
   ],
   "source": [
    "#grad = tf.train.GradientDescentOptimizer(image).compute_gradients(cost)\n",
    "# tf Graph Input\n",
    "# x = tf.placeholder(tf.float32, [None, 784]) # mnist data image of shape 28*28=784\n",
    "# y = tf.placeholder(tf.float32, [None, 10]) # 0-9 digits recognition => 10 classes\n",
    "\n",
    "#Getting an image\n",
    "dummy = mnist.train.images[0]\n",
    "x_ = dummy[np.newaxis,:]\n",
    "dummy_l = mnist.train.labels[0]\n",
    "y_ = dummy_l[np.newaxis,:]\n",
    "\n",
    "# Set model weights\n",
    "W = tf.Variable(tf.zeros([784, 10]))\n",
    "b = tf.Variable(tf.zeros([10]))\n",
    "\n",
    "# Construct model\n",
    "pred = tf.nn.softmax(tf.matmul(x_, W) + b) # Softmax\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2b92cf085c0>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(x_adv.reshape((28,28)), cmap='Greys')\n",
    "\n",
    "plt.imshow(img, cmap='Greys_r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 784)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fea1688d940>"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "adv_ = adv.reshape(1,784)\n",
    "print(adv_.shape)\n",
    "plt.imshow(adv_.reshape((28,28)), cmap='Greys')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(mnist.train.labels[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
