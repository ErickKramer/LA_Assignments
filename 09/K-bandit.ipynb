{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Implement a simulation of the k-ardmed bandits environment with a variable value of k and a random p_i probabilities to obtain a reward of 0 or 1 from pulling each machine. Probabilities should be different each time you instance the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bandit:\n",
    "    def __init__(self, num_bandits, bandit_probs):\n",
    "        self.k = num_bandits # Number of probabilities\n",
    "        self.probs = bandit_probs       \n",
    "        \n",
    "    def get_reward(self,action):\n",
    "        p_i = np.round(np.random.random(),2)\n",
    "        reward = 1 if (p_i < self.probs[action]) else 0\n",
    "#         print(' Reward calculated')\n",
    "        return reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, bandit, epsilon):\n",
    "        # epsilon to control the type of agent (Explorer or greedy)\n",
    "        self.epsilon = epsilon\n",
    "        # number of times action was chosen\n",
    "        self.n_actions = np.zeros(bandit.k, dtype=np.int) \n",
    "        # Estimated values? of what?\n",
    "        self.Q = np.zeros(bandit.k, dtype=np.float)\n",
    "    \n",
    "    # Update Q action-value based on the formula\n",
    "    # Q(a) = Q(a) + 1/(k+1) * (r(a) - Q(a))\n",
    "    def update_Q(self, action, reward):\n",
    "        self.n_actions[action] += 1\n",
    "        self.Q[action] += (1/self.n_actions[action]) * (reward - self.Q[action])\n",
    "        \n",
    "    def choose_action(self, bandit, force_explore=False):\n",
    "        rand = np.round(np.random.random(),2)\n",
    "#         print(' rand: ', rand)\n",
    "        if (rand < self.epsilon) or force_explore:\n",
    "#             print(' I wanna explore')\n",
    "            action_explore = np.random.randint(bandit.k) #Explore random bandits?\n",
    "            return action_explore\n",
    "        else:\n",
    "#             print(' I wanna stay')\n",
    "#             print(' Q', self.Q)\n",
    "#             print(' Q_max:', self.Q.max())\n",
    "#             print (' flat: ',np.flatnonzero(self.Q == self.Q.max()))\n",
    "            action_greedy = np.random.choice(np.flatnonzero(self.Q == self.Q.max()))\n",
    "            return action_greedy       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment(agent, bandit, N_episodes):\n",
    "        action_history = []\n",
    "        reward_history = []\n",
    "        \n",
    "        for episode in range(N_episodes):\n",
    "            # Choose action from agent \n",
    "            action = agent.choose_action(bandit)\n",
    "#             print(' Corresponding action: ', action)\n",
    "            reward = bandit.get_reward(action)\n",
    "#             print(' r: ', reward)\n",
    "            agent.update_Q(action, reward)\n",
    "            action_history.append(action)\n",
    "            reward_history.append(reward)\n",
    "        return np.array(action_history), np.array(reward_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Running bandits experiment with 10 bandits and agent with epsilon of 0.1\n",
      " Probabilities calculated [0.98, 0.38, 0.75, 0.37, 0.85, 0.58, 0.75, 0.19, 0.39, 0.59]\n",
      " Best bandit is  1\n",
      " Experiment:  0\n",
      " Reward history avg:  [1. 0. 0. 1. 0. 1. 1. 0. 0. 1. 0. 1. 1. 1. 0. 0. 1. 0. 1. 0. 1. 1. 0. 0.\n",
      " 0. 1. 1. 0. 1. 1. 1. 1. 0. 0. 1. 1. 0. 1. 0. 1. 1. 1. 1. 1. 1. 1. 0. 1.\n",
      " 1. 1. 1. 1. 1. 0. 0. 1. 1. 0. 0. 1. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 0. 1. 0. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0.\n",
      " 1. 1. 1. 1.]\n",
      " Experiment:  1\n",
      " Reward history avg:  [1. 0. 1. 2. 1. 1. 2. 1. 1. 1. 1. 2. 1. 2. 0. 1. 2. 1. 1. 1. 1. 2. 0. 1.\n",
      " 1. 1. 2. 1. 2. 2. 2. 2. 1. 1. 2. 2. 1. 2. 1. 2. 2. 2. 2. 2. 2. 2. 1. 2.\n",
      " 2. 2. 2. 1. 2. 1. 1. 2. 1. 1. 1. 2. 0. 1. 1. 1. 2. 2. 2. 2. 2. 2. 2. 2.\n",
      " 2. 2. 2. 2. 1. 1. 2. 1. 1. 2. 2. 2. 2. 0. 2. 2. 2. 2. 2. 2. 2. 2. 2. 1.\n",
      " 2. 2. 2. 2.]\n",
      " Experiment:  2\n",
      " Reward history avg:  [1. 0. 2. 3. 2. 2. 3. 2. 2. 2. 2. 3. 2. 3. 1. 2. 2. 2. 2. 2. 2. 3. 1. 2.\n",
      " 2. 2. 3. 2. 3. 3. 3. 3. 2. 2. 3. 3. 2. 3. 2. 3. 2. 3. 3. 3. 3. 3. 2. 2.\n",
      " 3. 3. 3. 2. 3. 2. 2. 3. 2. 2. 2. 3. 0. 2. 2. 1. 3. 3. 3. 3. 3. 3. 3. 3.\n",
      " 3. 3. 3. 3. 2. 2. 3. 1. 2. 3. 3. 3. 3. 1. 3. 2. 3. 3. 3. 3. 3. 3. 3. 2.\n",
      " 2. 3. 3. 3.]\n",
      " Experiment:  3\n",
      " Reward history avg:  [2. 1. 3. 3. 3. 3. 3. 3. 2. 3. 3. 4. 3. 4. 2. 3. 3. 2. 3. 3. 3. 4. 2. 3.\n",
      " 3. 3. 4. 3. 4. 4. 4. 4. 3. 3. 4. 4. 3. 4. 3. 4. 3. 4. 4. 4. 4. 4. 3. 3.\n",
      " 3. 4. 4. 3. 4. 3. 3. 4. 3. 3. 3. 4. 1. 3. 2. 2. 4. 4. 4. 4. 4. 4. 4. 4.\n",
      " 4. 4. 4. 4. 3. 3. 4. 2. 3. 4. 4. 4. 4. 2. 4. 3. 4. 4. 4. 4. 4. 4. 4. 3.\n",
      " 2. 4. 4. 4.]\n",
      " Experiment:  4\n",
      " Reward history avg:  [3. 2. 4. 3. 4. 4. 4. 4. 3. 4. 4. 5. 4. 4. 3. 4. 4. 3. 4. 4. 4. 5. 3. 4.\n",
      " 4. 4. 5. 4. 5. 5. 5. 5. 4. 4. 5. 5. 4. 5. 4. 5. 4. 5. 5. 5. 5. 5. 4. 4.\n",
      " 4. 5. 5. 4. 5. 4. 4. 5. 4. 4. 4. 5. 2. 4. 3. 3. 5. 5. 5. 4. 5. 5. 5. 5.\n",
      " 5. 5. 5. 5. 4. 4. 5. 3. 4. 5. 5. 5. 5. 3. 5. 4. 4. 5. 5. 5. 5. 5. 5. 4.\n",
      " 3. 5. 5. 5.]\n",
      " Experiment:  5\n",
      " Reward history avg:  [4. 2. 4. 3. 5. 4. 4. 4. 3. 5. 5. 6. 5. 4. 4. 4. 4. 4. 4. 5. 5. 5. 3. 4.\n",
      " 5. 4. 5. 4. 5. 6. 5. 6. 5. 4. 6. 5. 4. 5. 4. 5. 4. 6. 5. 5. 6. 5. 4. 4.\n",
      " 4. 5. 6. 5. 6. 4. 5. 6. 5. 5. 5. 6. 2. 5. 4. 4. 5. 6. 5. 5. 5. 6. 6. 6.\n",
      " 5. 6. 6. 5. 5. 5. 6. 3. 5. 6. 6. 6. 5. 4. 6. 4. 4. 6. 6. 5. 5. 6. 6. 4.\n",
      " 4. 5. 6. 6.]\n",
      " Experiment:  6\n",
      " Reward history avg:  [4. 2. 5. 3. 6. 5. 5. 5. 4. 6. 6. 7. 6. 5. 5. 4. 5. 5. 5. 5. 6. 5. 4. 5.\n",
      " 6. 5. 5. 5. 6. 7. 6. 7. 6. 4. 7. 6. 5. 6. 5. 6. 5. 7. 6. 6. 7. 6. 5. 5.\n",
      " 5. 6. 7. 6. 7. 5. 6. 6. 6. 6. 6. 7. 3. 6. 5. 5. 6. 7. 6. 6. 6. 6. 7. 7.\n",
      " 6. 7. 7. 6. 6. 6. 7. 4. 6. 7. 7. 7. 6. 5. 7. 4. 5. 7. 7. 6. 6. 7. 7. 5.\n",
      " 4. 6. 7. 7.]\n",
      " Experiment:  7\n",
      " Reward history avg:  [5. 3. 5. 4. 7. 6. 6. 5. 5. 7. 6. 8. 6. 6. 6. 5. 6. 6. 6. 6. 7. 5. 5. 6.\n",
      " 7. 6. 6. 5. 6. 7. 6. 7. 7. 4. 8. 7. 5. 7. 6. 7. 6. 8. 6. 6. 8. 7. 6. 5.\n",
      " 6. 7. 8. 6. 8. 6. 6. 7. 6. 7. 6. 8. 4. 6. 6. 6. 7. 7. 7. 6. 7. 6. 8. 8.\n",
      " 7. 8. 8. 6. 7. 7. 8. 4. 6. 7. 8. 8. 6. 6. 8. 5. 6. 7. 8. 6. 6. 8. 8. 6.\n",
      " 5. 7. 7. 8.]\n",
      " Experiment:  8\n",
      " Reward history avg:  [6. 4. 5. 4. 7. 7. 7. 5. 6. 7. 7. 8. 6. 6. 7. 5. 7. 6. 7. 6. 7. 6. 5. 7.\n",
      " 7. 7. 6. 5. 7. 7. 6. 8. 7. 4. 9. 8. 5. 7. 6. 8. 6. 8. 6. 7. 8. 7. 7. 5.\n",
      " 7. 8. 8. 6. 8. 6. 6. 8. 7. 8. 6. 9. 4. 6. 6. 6. 8. 7. 8. 6. 7. 6. 8. 8.\n",
      " 7. 8. 9. 6. 8. 7. 8. 4. 6. 7. 8. 8. 6. 6. 8. 5. 7. 8. 8. 7. 6. 8. 9. 7.\n",
      " 5. 7. 8. 8.]\n",
      " Experiment:  9\n",
      " Reward history avg:  [ 6.  4.  5.  5.  8.  8.  8.  6.  7.  8.  8.  9.  7.  7.  8.  6.  8.  7.\n",
      "  8.  6.  8.  7.  6.  8.  8.  8.  7.  6.  8.  8.  7.  9.  8.  5. 10.  9.\n",
      "  6.  8.  7.  9.  7.  9.  7.  8.  9.  7.  7.  6.  8.  9.  9.  7.  9.  7.\n",
      "  7.  9.  8.  9.  6. 10.  4.  7.  7.  7.  9.  8.  9.  7.  8.  7.  9.  9.\n",
      "  8.  9. 10.  7.  8.  8.  9.  5.  7.  8.  9.  9.  7.  7.  9.  6.  8.  9.\n",
      "  8.  8.  7.  9. 10.  8.  6.  7.  9.  9.]\n",
      " Experiment:  10\n",
      " Reward history avg:  [ 6.  5.  6.  5.  9.  9.  9.  7.  8.  9.  9. 10.  8.  8.  8.  7.  9.  8.\n",
      "  9.  7.  9.  8.  7.  9.  9.  9.  8.  7.  9.  9.  8. 10.  9.  6. 11. 10.\n",
      "  7.  9.  7. 10.  8. 10.  8.  9. 10.  8.  8.  7.  9. 10. 10.  8. 10.  8.\n",
      "  8. 10.  9. 10.  7. 11.  4.  8.  8.  8. 10.  9. 10.  8.  9.  8. 10. 10.\n",
      "  9. 10. 11.  8.  9.  9. 10.  5.  8.  9. 10. 10.  8.  8. 10.  7.  9. 10.\n",
      "  9.  9.  8. 10. 11.  9.  7.  8.  9. 10.]\n",
      " Experiment:  11\n",
      " Reward history avg:  [ 7.  5.  6.  5. 10.  9. 10.  7.  9.  9.  9. 11.  8.  9.  9.  7.  9.  8.\n",
      "  9.  8. 10.  8.  7. 10. 10. 10.  9.  8. 10.  9.  9. 11. 10.  6. 12. 10.\n",
      "  8.  9.  8. 11.  9. 11.  9. 10. 11.  8.  9.  8. 10. 10. 10.  9. 11.  9.\n",
      "  9. 11. 10. 11.  8. 11.  5.  9.  9.  8. 11. 10. 11.  9. 10.  9. 11. 11.\n",
      " 10. 11. 12.  9.  9. 10. 11.  6.  9. 10. 11. 11.  9.  9. 11.  8.  9. 11.\n",
      " 10. 10.  9. 11. 12. 10.  8.  8. 10. 11.]\n",
      " Experiment:  12\n",
      " Reward history avg:  [ 7.  5.  6.  6. 11. 10. 11.  8. 10. 10. 10. 12.  9. 10. 10.  8. 10.  9.\n",
      " 10.  9. 11.  9.  8. 11. 11. 11.  9.  9. 11. 10. 10. 12. 11.  7. 13. 11.\n",
      "  9. 10.  9. 12. 10. 12. 10. 11. 12.  9. 10.  9. 11. 11. 11. 10. 12. 10.\n",
      " 10. 12. 11. 12.  9. 12.  6.  9. 10.  9. 12. 11. 12. 10. 11. 10. 12. 12.\n",
      " 11. 12. 13. 10. 10. 10. 12.  6. 10. 11. 12. 12.  9. 10. 12.  9. 10. 12.\n",
      " 11. 11. 10. 12. 13. 11.  9.  9. 11. 12.]\n",
      " Experiment:  13\n",
      " Reward history avg:  [ 8.  5.  7.  7. 12. 11. 12.  9. 11. 11. 11. 13. 10. 11. 11.  9. 11. 10.\n",
      " 11. 10. 12.  9.  8. 12. 12. 12. 10. 10. 12. 10. 11. 13. 12.  8. 14. 11.\n",
      " 10. 11. 10. 13. 11. 13. 11. 12. 13. 10. 11. 10. 12. 12. 12. 11. 13. 11.\n",
      " 11. 13. 12. 13. 10. 13.  7. 10. 11. 10. 13. 12. 13. 11. 12. 11. 13. 13.\n",
      " 12. 13. 14. 11. 11. 11. 13.  7. 11. 12. 13. 13. 10. 11. 13. 10. 11. 13.\n",
      " 12. 12. 11. 13. 14. 11. 10. 10. 12. 13.]\n",
      " Experiment:  14\n",
      " Reward history avg:  [ 9.  6.  7.  7. 12. 12. 13.  9. 11. 12. 12. 13. 10. 12. 12. 10. 12. 11.\n",
      " 12. 10. 12. 10.  9. 13. 12. 13. 11. 11. 12. 11. 11. 14. 12.  8. 15. 12.\n",
      " 11. 12. 11. 14. 11. 14. 12. 13. 14. 11. 11. 11. 13. 13. 12. 11. 14. 12.\n",
      " 12. 14. 13. 14. 10. 13.  8. 11. 12. 11. 14. 13. 14. 12. 13. 12. 14. 14.\n",
      " 12. 14. 15. 12. 11. 11. 13.  7. 12. 12. 14. 14. 11. 12. 14. 10. 11. 14.\n",
      " 13. 13. 11. 14. 15. 11. 10. 11. 13. 14.]\n",
      " Experiment:  15\n",
      " Reward history avg:  [ 9.  6.  7.  7. 13. 12. 13.  9. 12. 12. 13. 14. 10. 12. 13. 10. 13. 12.\n",
      " 13. 10. 13. 11. 10. 14. 13. 14. 11. 12. 13. 12. 12. 14. 13.  9. 16. 13.\n",
      " 12. 13. 12. 14. 12. 14. 13. 14. 14. 12. 12. 12. 14. 14. 13. 12. 15. 13.\n",
      " 13. 14. 13. 15. 11. 14.  9. 11. 13. 12. 14. 14. 14. 13. 14. 13. 14. 15.\n",
      " 13. 14. 16. 13. 12. 12. 14.  8. 12. 13. 14. 15. 12. 13. 14. 11. 12. 15.\n",
      " 13. 14. 12. 15. 16. 12. 10. 12. 14. 14.]\n",
      " Experiment:  16\n",
      " Reward history avg:  [ 9.  7.  8.  7. 14. 13. 14. 10. 13. 13. 14. 15. 11. 13. 13. 11. 14. 13.\n",
      " 14. 11. 14. 12. 10. 15. 14. 14. 12. 13. 14. 12. 13. 14. 14. 10. 17. 14.\n",
      " 13. 14. 13. 15. 13. 14. 14. 15. 15. 13. 13. 13. 15. 14. 13. 13. 16. 14.\n",
      " 14. 15. 14. 16. 12. 15. 10. 12. 14. 13. 15. 15. 15. 14. 14. 14. 15. 16.\n",
      " 14. 15. 17. 14. 13. 13. 15.  8. 13. 14. 15. 16. 13. 14. 15. 12. 13. 16.\n",
      " 14. 14. 13. 16. 17. 12. 11. 13. 15. 15.]\n",
      " Experiment:  17\n",
      " Reward history avg:  [ 9.  7.  8.  8. 15. 14. 15. 10. 13. 14. 15. 16. 11. 14. 14. 12. 15. 14.\n",
      " 15. 12. 15. 13. 11. 16. 15. 15. 13. 14. 15. 12. 14. 15. 15. 11. 18. 14.\n",
      " 14. 15. 14. 15. 14. 15. 14. 16. 16. 14. 14. 14. 16. 14. 14. 14. 17. 14.\n",
      " 15. 15. 15. 16. 13. 16. 11. 12. 15. 14. 16. 16. 15. 15. 15. 14. 16. 17.\n",
      " 15. 16. 18. 15. 14. 14. 15.  9. 14. 15. 16. 16. 14. 15. 16. 13. 14. 17.\n",
      " 15. 15. 14. 17. 18. 13. 11. 13. 16. 16.]\n",
      " Experiment:  18\n",
      " Reward history avg:  [ 9.  8.  9.  9. 15. 15. 16. 11. 14. 15. 16. 17. 12. 14. 14. 13. 16. 15.\n",
      " 16. 13. 16. 14. 11. 17. 16. 16. 14. 15. 16. 12. 15. 16. 16. 12. 19. 15.\n",
      " 14. 16. 15. 15. 15. 16. 15. 17. 17. 15. 15. 15. 17. 15. 15. 15. 18. 15.\n",
      " 16. 16. 16. 17. 14. 17. 12. 13. 15. 15. 17. 17. 16. 15. 16. 15. 17. 18.\n",
      " 16. 16. 19. 16. 15. 15. 16. 10. 15. 15. 17. 16. 15. 15. 17. 14. 15. 18.\n",
      " 16. 16. 15. 17. 19. 13. 12. 14. 17. 17.]\n",
      " Experiment:  19\n",
      " Reward history avg:  [10.  9.  9. 10. 16. 15. 17. 11. 15. 15. 16. 17. 13. 14. 15. 14. 17. 16.\n",
      " 17. 14. 17. 15. 12. 18. 17. 17. 15. 15. 17. 13. 16. 17. 17. 13. 20. 15.\n",
      " 15. 17. 16. 16. 16. 17. 16. 18. 18. 16. 16. 16. 18. 16. 16. 16. 19. 16.\n",
      " 17. 17. 17. 18. 14. 18. 13. 14. 16. 16. 18. 18. 17. 16. 17. 16. 18. 19.\n",
      " 17. 17. 20. 17. 16. 16. 17. 10. 16. 16. 18. 17. 16. 16. 17. 15. 16. 19.\n",
      " 17. 17. 16. 17. 20. 14. 13. 15. 18. 18.]\n",
      " reward history avg = [0.5  0.45 0.45 0.5  0.8  0.75 0.85 0.55 0.75 0.75 0.8  0.85 0.65 0.7\n",
      " 0.75 0.7  0.85 0.8  0.85 0.7  0.85 0.75 0.6  0.9  0.85 0.85 0.75 0.75\n",
      " 0.85 0.65 0.8  0.85 0.85 0.65 1.   0.75 0.75 0.85 0.8  0.8  0.8  0.85\n",
      " 0.8  0.9  0.9  0.8  0.8  0.8  0.9  0.8  0.8  0.8  0.95 0.8  0.85 0.85\n",
      " 0.85 0.9  0.7  0.9  0.65 0.7  0.8  0.8  0.9  0.9  0.85 0.8  0.85 0.8\n",
      " 0.9  0.95 0.85 0.85 1.   0.85 0.8  0.8  0.85 0.5  0.8  0.8  0.9  0.85\n",
      " 0.8  0.8  0.85 0.75 0.8  0.95 0.85 0.85 0.8  0.85 1.   0.7  0.65 0.75\n",
      " 0.9  0.9 ]\n",
      " action history avg = [[ 2.  5.  1.  0.  0.  4.  1.  4.  2.  1.]\n",
      " [ 3.  3.  1.  3.  1.  4.  0.  3.  1.  1.]\n",
      " [ 2.  3.  2.  0.  2.  2.  3.  3.  1.  2.]\n",
      " [ 4.  3.  1.  0.  1.  3.  5.  1.  1.  1.]\n",
      " [ 8.  3.  3.  0.  1.  2.  3.  0.  0.  0.]\n",
      " [ 9.  2.  1.  1.  1.  3.  3.  0.  0.  0.]\n",
      " [ 8.  2.  0.  1.  1.  3.  4.  0.  0.  1.]\n",
      " [ 8.  2.  1.  1.  1.  3.  3.  1.  0.  0.]\n",
      " [ 8.  3.  1.  1.  1.  3.  2.  0.  0.  1.]\n",
      " [ 8.  1.  1.  2.  1.  3.  2.  0.  0.  2.]\n",
      " [ 8.  1.  1.  3.  1.  3.  3.  0.  0.  0.]\n",
      " [ 8.  1.  1.  2.  1.  3.  4.  0.  0.  0.]\n",
      " [ 7.  1.  2.  3.  1.  2.  3.  1.  0.  0.]\n",
      " [ 7.  2.  1.  2.  2.  2.  3.  1.  0.  0.]\n",
      " [ 8.  2.  1.  3.  1.  2.  3.  0.  0.  0.]\n",
      " [ 7.  1.  2.  2.  2.  2.  4.  0.  0.  0.]\n",
      " [ 7.  0.  1.  2.  2.  2.  5.  0.  0.  1.]\n",
      " [ 7.  0.  2.  2.  2.  2.  5.  0.  0.  0.]\n",
      " [ 9.  0.  1.  2.  1.  2.  5.  0.  0.  0.]\n",
      " [10.  0.  0.  2.  1.  2.  5.  0.  0.  0.]\n",
      " [ 9.  0.  1.  2.  1.  2.  3.  1.  0.  1.]\n",
      " [ 8.  0.  2.  2.  2.  2.  4.  0.  0.  0.]\n",
      " [ 8.  1.  1.  2.  2.  1.  4.  0.  0.  1.]\n",
      " [10.  0.  1.  2.  1.  2.  4.  0.  0.  0.]\n",
      " [10.  0.  2.  2.  1.  1.  4.  0.  0.  0.]\n",
      " [ 9.  0.  2.  2.  2.  1.  3.  0.  0.  1.]\n",
      " [10.  0.  2.  2.  0.  1.  4.  0.  0.  1.]\n",
      " [10.  0.  0.  2.  0.  2.  4.  1.  0.  1.]\n",
      " [11.  0.  1.  2.  1.  1.  3.  0.  0.  1.]\n",
      " [ 9.  0.  1.  2.  0.  1.  4.  0.  0.  3.]\n",
      " [10.  0.  1.  2.  1.  1.  5.  0.  0.  0.]\n",
      " [10.  0.  1.  2.  1.  1.  4.  1.  0.  0.]\n",
      " [11.  0.  1.  2.  1.  1.  4.  0.  0.  0.]\n",
      " [10.  0.  1.  2.  1.  1.  5.  0.  0.  0.]\n",
      " [10.  0.  1.  4.  1.  1.  3.  0.  0.  0.]\n",
      " [ 9.  0.  1.  3.  1.  1.  4.  1.  0.  0.]\n",
      " [10.  0.  1.  2.  2.  1.  4.  0.  0.  0.]\n",
      " [11.  0.  1.  2.  1.  1.  4.  0.  0.  0.]\n",
      " [10.  1.  1.  2.  1.  1.  4.  0.  0.  0.]\n",
      " [11.  0.  1.  2.  1.  1.  4.  0.  0.  0.]\n",
      " [10.  0.  1.  2.  1.  1.  4.  0.  1.  0.]\n",
      " [10.  0.  1.  2.  2.  1.  4.  0.  0.  0.]\n",
      " [10.  0.  1.  2.  1.  1.  5.  0.  0.  0.]\n",
      " [10.  0.  2.  1.  1.  1.  4.  1.  0.  0.]\n",
      " [10.  0.  1.  1.  4.  1.  3.  0.  0.  0.]\n",
      " [10.  0.  2.  1.  3.  1.  3.  0.  0.  0.]\n",
      " [ 9.  0.  1.  2.  3.  1.  4.  0.  0.  0.]\n",
      " [10.  0.  1.  2.  1.  1.  4.  0.  1.  0.]\n",
      " [ 9.  0.  1.  2.  2.  1.  4.  0.  1.  0.]\n",
      " [10.  0.  0.  3.  1.  1.  4.  0.  1.  0.]\n",
      " [10.  1.  2.  1.  2.  1.  2.  0.  0.  1.]\n",
      " [10.  0.  2.  2.  1.  1.  3.  0.  0.  1.]\n",
      " [11.  0.  2.  1.  1.  1.  3.  0.  0.  1.]\n",
      " [11.  0.  1.  2.  1.  1.  3.  0.  0.  1.]\n",
      " [ 9.  0.  2.  1.  3.  1.  3.  0.  0.  1.]\n",
      " [ 8.  0.  2.  2.  3.  1.  3.  0.  0.  1.]\n",
      " [ 8.  0.  1.  1.  3.  1.  4.  0.  1.  1.]\n",
      " [ 9.  0.  0.  2.  3.  0.  4.  1.  0.  1.]\n",
      " [ 8.  0.  1.  1.  4.  1.  4.  0.  0.  1.]\n",
      " [10.  0.  1.  1.  2.  1.  4.  0.  0.  1.]\n",
      " [ 7.  0.  2.  1.  2.  1.  5.  0.  1.  1.]\n",
      " [10.  0.  1.  0.  1.  2.  4.  1.  0.  1.]\n",
      " [10.  0.  1.  0.  1.  1.  5.  1.  0.  1.]\n",
      " [11.  2.  1.  0.  2.  1.  2.  0.  0.  1.]\n",
      " [12.  0.  1.  1.  2.  0.  3.  0.  1.  0.]\n",
      " [12.  0.  1.  1.  2.  0.  3.  0.  0.  1.]\n",
      " [12.  0.  0.  1.  3.  0.  3.  0.  0.  1.]\n",
      " [12.  1.  1.  1.  0.  0.  3.  1.  0.  1.]\n",
      " [12.  0.  1.  0.  1.  1.  3.  0.  1.  1.]\n",
      " [12.  0.  1.  1.  1.  1.  2.  0.  1.  1.]\n",
      " [11.  0.  1.  1.  3.  1.  2.  0.  0.  1.]\n",
      " [13.  0.  0.  1.  1.  1.  3.  0.  0.  1.]\n",
      " [13.  0.  1.  1.  1.  1.  2.  0.  0.  1.]\n",
      " [13.  0.  0.  1.  1.  1.  2.  0.  0.  2.]\n",
      " [11.  0.  1.  1.  1.  1.  3.  1.  0.  1.]\n",
      " [13.  1.  0.  1.  0.  1.  3.  0.  0.  1.]\n",
      " [11.  1.  0.  1.  0.  2.  3.  1.  0.  1.]\n",
      " [13.  0.  1.  1.  0.  1.  3.  0.  0.  1.]\n",
      " [13.  0.  1.  1.  0.  0.  4.  0.  0.  1.]\n",
      " [ 9.  0.  1.  1.  0.  2.  4.  0.  1.  2.]\n",
      " [11.  0.  1.  1.  0.  1.  5.  0.  0.  1.]\n",
      " [13.  0.  0.  1.  0.  0.  5.  0.  0.  1.]\n",
      " [13.  0.  0.  1.  0.  0.  4.  0.  0.  2.]\n",
      " [12.  0.  2.  0.  1.  0.  4.  0.  0.  1.]\n",
      " [11.  0.  1.  1.  2.  0.  4.  0.  0.  1.]\n",
      " [ 9.  1.  1.  1.  1.  0.  3.  0.  1.  3.]\n",
      " [11.  0.  1.  1.  1.  0.  3.  0.  0.  3.]\n",
      " [11.  1.  0.  1.  1.  0.  3.  1.  0.  2.]\n",
      " [12.  0.  1.  0.  0.  0.  2.  0.  1.  4.]\n",
      " [13.  1.  0.  0.  0.  0.  2.  0.  1.  3.]\n",
      " [12.  0.  0.  0.  0.  0.  4.  1.  1.  2.]\n",
      " [13.  0.  0.  0.  0.  0.  4.  0.  1.  2.]\n",
      " [14.  0.  0.  0.  0.  0.  3.  0.  1.  2.]\n",
      " [12.  0.  0.  2.  0.  0.  4.  0.  0.  2.]\n",
      " [13.  1.  0.  0.  0.  0.  3.  0.  1.  2.]\n",
      " [11.  1.  1.  0.  0.  0.  3.  0.  2.  2.]\n",
      " [12.  0.  0.  0.  0.  0.  4.  0.  2.  2.]\n",
      " [14.  0.  0.  0.  0.  0.  4.  0.  1.  1.]\n",
      " [11.  1.  0.  0.  1.  0.  3.  1.  2.  1.]\n",
      " [14.  0.  0.  0.  0.  0.  4.  0.  1.  1.]]\n"
     ]
    }
   ],
   "source": [
    "# --------------------- \n",
    "#  k-bandit simulation           \n",
    "# --------------------- \n",
    "\n",
    "# number of bandits\n",
    "num_bandits = 10\n",
    "epsilon = 0.1\n",
    "n_episodes = 100\n",
    "n_experiments = 20\n",
    "\n",
    "print(' Running bandits experiment with {} bandits and agent with epsilon of {}'.format(num_bandits, epsilon))\n",
    "\n",
    "# for i in xrange(n_experiments):\n",
    "\n",
    "# Do we need history?\n",
    "reward_history_avg = np.zeros(n_episodes)\n",
    "action_history_sum = np.zeros((n_episodes, num_bandits))\n",
    "\n",
    "bandit_probs = [ np.round(np.random.rand(),2) for i in range(num_bandits)]\n",
    "\n",
    "best_bandit = np.argmax(bandit_probs)\n",
    "print(' Probabilities calculated', bandit_probs)\n",
    "print(' Best bandit is ', best_bandit+1)\n",
    "\n",
    "for ex in range(n_experiments):\n",
    "    print(' Experiment: ', ex)\n",
    "    bandit = Bandit(num_bandits,bandit_probs) # Initialize bandits\n",
    "    agent = Agent(bandit, epsilon) # Initialize agents\n",
    "\n",
    "    action_history, reward_history = experiment(agent, bandit, n_episodes)\n",
    "\n",
    "#     print(' Action history: ', action_history)\n",
    "\n",
    "    # Sum up experiment reward \n",
    "    reward_history_avg += reward_history\n",
    "    print(' Reward history avg: ', reward_history_avg)\n",
    "\n",
    "    # Sum up action history\n",
    "    for episodes_idx, (a) in enumerate(action_history): \n",
    "        action_history_sum[episodes_idx][a] += 1\n",
    "    \n",
    "reward_history_avg /= np.float(n_experiments)\n",
    "\n",
    "print(' reward history avg = {}'.format(reward_history_avg))\n",
    "\n",
    "\n",
    "print(' action history avg = {}'.format(action_history_sum))\n",
    "# #------------------------------\n",
    "# # Plot reward history\n",
    "# #------------------------------\n",
    "# plt.figure(figsize=(18, 12))\n",
    "# plt.plot(reward_history_avg)\n",
    "# plt.xlabel('Episode number')\n",
    "# plt.ylabel('Rewards collected'.format(n_experiments))\n",
    "# plt.title('Bandit reward history averaged over {} experiments(epsilon = {})'.format(n_experiments, epsilon))\n",
    "# ax = plt.gca()\n",
    "# ax.set_xscale('log', nonposx='clip')\n",
    "# plt.xlim([1, n_episodes])\n",
    "# plt.show()\n",
    "\n",
    "# # =========================\n",
    "# # Plot action history results\n",
    "# # =========================\n",
    "# plt.figure(figsize=(18, 12))\n",
    "# for i in range(num_bandits):\n",
    "#     action_history_sum_plot = 100 * action_history_sum[:,i] / n_experiments\n",
    "#     plt.plot(list(np.array(range(len(action_history_sum_plot)))+1),\n",
    "#                  action_history_sum_plot,\n",
    "#                  linewidth=5.0,\n",
    "#                  label=\"Bandit #{}\".format(i+1))\n",
    "# plt.title(\"Bandit action history averaged over {} experiments (epsilon = {})\".format(n_experiments, epsilon), fontsize=26)\n",
    "# plt.xlabel(\"Episode Number\", fontsize=26)\n",
    "# plt.ylabel(\"Bandit Action Choices (%)\", fontsize=26)\n",
    "# leg = plt.legend(loc='upper left', shadow=True, fontsize=26)\n",
    "# ax = plt.gca()\n",
    "# ax.set_xscale(\"log\", nonposx='clip')\n",
    "# plt.xlim([1, n_episodes])\n",
    "# plt.ylim([0, 100])\n",
    "# plt.xticks(fontsize=24)\n",
    "# plt.yticks(fontsize=24)\n",
    "# for legobj in leg.legendHandles:\n",
    "#     legobj.set_linewidth(16.0)\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.97333333, 0.        , 0.        , 0.        , 0.85714286,\n",
       "       0.5       , 0.33333333, 0.        , 0.        , 0.5       ])"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.Q"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
